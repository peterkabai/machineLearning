{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Harry Potter Text Gen.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "RuJ2bU0Epw71",
        "colab_type": "code",
        "outputId": "03ed7023-3ca6-42b0-9192-d1be6e7bac21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print('GPU device not found')\n",
        "  print('Simply select \"GPU\" in the Accelerator drop-down in Notebook Settings')\n",
        "else:\n",
        "  print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "L26KLhkkkphB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import requests\n",
        "url = \"https://raw.githubusercontent.com/peterkabai/tensorFlow/master/textFiles/sorcerersStone.txt\"\n",
        "data = requests.get(url).text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hGeLSj7sGCrk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "chars = list(set(data))\n",
        "data_size, vocab_size = len(data), len(chars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Zc7tFeBSjVq9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# the number of sequential characters to sample\n",
        "length_of_input = 12\n",
        "\n",
        "# maps each character to a number and each number to a character\n",
        "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
        "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
        "\n",
        "# create training sequences and corresponding labels\n",
        "import numpy as np\n",
        "X = []\n",
        "y = []\n",
        "for i in range(0, len(data)-length_of_input-1, 1):\n",
        "  X.append([char_to_ix[ch] for ch in data[i:i+length_of_input]])\n",
        "  y.append([char_to_ix[ch] for ch in data[i+1:i+length_of_input+1]])\n",
        "\n",
        "# reshapes the data\n",
        "X_modified = np.reshape(X, (len(X), length_of_input))\n",
        "y_modified = np.reshape(y, (len(y), length_of_input))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6ImKltoXqc_d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "import random\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# sets hyperparameters to mimic the Karpathy code\n",
        "n_neurons = 100\n",
        "num_batches = 10\n",
        "iterations = 200\n",
        "n_layers = 2\n",
        "learning_rate = 0.001\n",
        "sequence_to_use = 0\n",
        "num_sequences = X_modified.shape[0]\n",
        "epochs_run = 0\n",
        "\n",
        "# X has any number of batches and chars, and vocab_size due to one-hot encoding\n",
        "X = tf.placeholder(tf.float32, [None, None, vocab_size])\n",
        "\n",
        "# y has any number of batches, and length_of_input characters\n",
        "y = tf.placeholder(tf.int32, [None, length_of_input])\n",
        "\n",
        "# more TensorFlow stuff defined here\n",
        "layers = [tf.contrib.rnn.LSTMBlockCell(num_units=n_neurons) for layer in range(n_layers)]\n",
        "multi_layer_cell = tf.contrib.rnn.MultiRNNCell(layers)\n",
        "outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, X, dtype=tf.float32)\n",
        "logits = tf.layers.dense(outputs, vocab_size)\n",
        "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
        "loss = tf.reduce_mean(xentropy)\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "training_op = optimizer.minimize(loss)\n",
        "probs = tf.nn.softmax(logits)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Pi7bs2uvAQw5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "epoch_count = 0\n",
        "# function to front the characters that match an array of indices\n",
        "def print_chars(indexArray):\n",
        "    string = \"\"\n",
        "    for c in indexArray:\n",
        "        if (c != None):\n",
        "            string += ix_to_char.get(c)\n",
        "    print(string)\n",
        "    \n",
        "startChars = [\n",
        "    \"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\n",
        "    \"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\n",
        "    \"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\"\n",
        "]\n",
        "startSize = len(startChars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FEQFHopw1Bx9",
        "colab_type": "code",
        "outputId": "3080fed1-efe8-4b25-b2df-f19e965910b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "cell_type": "code",
      "source": [
        "init = tf.global_variables_initializer()\n",
        "print_every = 100\n",
        "with tf.Session() as sess:\n",
        "    init.run()\n",
        "    \n",
        "    print(\"Encoding started...\")\n",
        "    X_encoded = tf.one_hot(X_modified, vocab_size)\n",
        "    X_encoded = X_encoded.eval()\n",
        "    print(\"Encoding done!\")\n",
        "    \n",
        "    # epoch loop starts here\n",
        "    while (True):\n",
        "        \n",
        "        if (epoch_count % print_every == 0):\n",
        "            # creates a seed to start the string from\n",
        "            pred_indices = [char_to_ix.get(startChars[random.randint(0,startSize-1)])]\n",
        "        \n",
        "        # each iteration is one new character\n",
        "        for iteraton in range(0, iterations):\n",
        "            \n",
        "            # creates batches, each row is an array of sequential characters\n",
        "            in_indices = []\n",
        "            out_indices = []\n",
        "            for batch in range(0, num_batches):\n",
        "            \n",
        "                # if we run out of sequences, the sequence to use returns to 0\n",
        "                if (sequence_to_use >= num_sequences):\n",
        "                    sequence_to_use = 0\n",
        "                    \n",
        "                # in and out indices are appended to\n",
        "                # 'sequence_to_use' is incrimented to get the next sequence when re-run\n",
        "                in_indices.append(X_encoded[sequence_to_use])\n",
        "                out_indices.append(y_modified[sequence_to_use])\n",
        "                sequence_to_use += 1\n",
        "            \n",
        "            # run the trainining op\n",
        "            sess.run(training_op, feed_dict={X: np.asarray(in_indices), y: np.asarray(out_indices)})\n",
        "        \n",
        "            if (epoch_count % print_every == 0):\n",
        "                # one hot encode the prediction indices\n",
        "                pred_encoded = tf.one_hot(np.asarray(pred_indices), vocab_size).eval()\n",
        "                \n",
        "                # get predictions as probabilities\n",
        "                predictions = sess.run(probs, feed_dict={X: np.asarray([pred_encoded])})\n",
        "                \n",
        "                # take the probabilities from the last character\n",
        "                # pick the next index using the probabilities\n",
        "                ix = np.random.choice(range(vocab_size), p=(predictions[0][-1]).ravel())\n",
        "                \n",
        "                # add to the array of indices\n",
        "                pred_indices.append(ix)\n",
        "                \n",
        "        if (epoch_count % print_every == 0):\n",
        "            # print the string every epoch for the first 'iterations_to_print' epochs\n",
        "            print(\"\\nEpoch:\", epoch_count,\"---------------------------------------------\")\n",
        "            print_chars(pred_indices)\n",
        "        \n",
        "        # increment the epoch count used to see when to print the results\n",
        "        epoch_count = epoch_count + 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoding started...\n",
            "Encoding done!\n",
            "\n",
            "Epoch: 0 ---------------------------------------------\n",
            "NsWDpoxU,F1gn,6jtmEuw  , tey   tt    tcsBot id mhh lcseansdaimBrd tds  .wn rwesh  in  reyiieer tlahet o etreoaimud ereneust.edrssosdidtnt tuyndteasy rsei ity r ihd rdnolaafh  y nDloy uyuoeoyl rr'aDo hh\n",
            "\n",
            "Epoch: 100 ---------------------------------------------\n",
            "At mile glaim,\" \n",
            "Professor Fbrong dorn and nape it as an up a poind in in on the oxle romess was palled up ever at fon Potter is is is ands. \n",
            "Gleffants, but it's pikering drice?\" se gate she reeping \n",
            "\n",
            "Epoch: 200 ---------------------------------------------\n",
            "The bully,\" sait Harry. \n",
            "\"At was ening if their wordetionly a fack. He robecing his head and sured... he boked him from with thing to that Peevenge befo the mirecting reoply at her a spen the around t\n",
            "\n",
            "Epoch: 300 ---------------------------------------------\n",
            "Hermione afred into pale sweetes sout over expects. Sinside as to gran's onved out of me so tiekhctory's going to steared, but I went and side up You hing needs and -- 'lid on the carge lack over the o\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}