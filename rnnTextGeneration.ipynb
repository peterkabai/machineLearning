{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Harry Potter Text Gen.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "KXTJr4ME4bUM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Generating Text from Harry Potter and the Sorcerer's Stone\n",
        "### Start the Colab GPU"
      ]
    },
    {
      "metadata": {
        "id": "RuJ2bU0Epw71",
        "colab_type": "code",
        "outputId": "03ed7023-3ca6-42b0-9192-d1be6e7bac21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print('GPU device not found')\n",
        "  print('Simply select \"GPU\" in the Accelerator drop-down in Notebook Settings')\n",
        "else:\n",
        "  print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "daHWK9Js4q3B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Setup\n",
        "#### First, the text file is imported:"
      ]
    },
    {
      "metadata": {
        "id": "L26KLhkkkphB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import requests\n",
        "url = \"https://raw.githubusercontent.com/peterkabai/tensorFlow/master/textFiles/sorcerersStone.txt\"\n",
        "data = requests.get(url).text\n",
        "display(Markdown(\">\" + data[:298]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ooz844Q_4uV_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Then a list of characters is created. Below is some info about the character counts:"
      ]
    },
    {
      "metadata": {
        "id": "hGeLSj7sGCrk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "chars = list(set(data))\n",
        "data_size, vocab_size = len(data), len(chars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XiSXS6ao5KoN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Here sequences of characters are generated, characters are mapped to indices and indices to characters"
      ]
    },
    {
      "metadata": {
        "id": "Zc7tFeBSjVq9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# the number of sequential characters to sample\n",
        "length_of_input = 12\n",
        "\n",
        "# maps each character to a number and each number to a character\n",
        "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
        "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
        "\n",
        "# create training sequences and corresponding labels\n",
        "import numpy as np\n",
        "X = []\n",
        "y = []\n",
        "for i in range(0, len(data)-length_of_input-1, 1):\n",
        "  X.append([char_to_ix[ch] for ch in data[i:i+length_of_input]])\n",
        "  y.append([char_to_ix[ch] for ch in data[i+1:i+length_of_input+1]])\n",
        "\n",
        "# reshapes the data\n",
        "X_modified = np.reshape(X, (len(X), length_of_input))\n",
        "y_modified = np.reshape(y, (len(y), length_of_input))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SJALSVjl5hOu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Finally, a function is created to convert indeces to characters and print the result, and starting characters are declared. When predicting sequences the starting character will be randomly choosen from these starting characters."
      ]
    },
    {
      "metadata": {
        "id": "Pi7bs2uvAQw5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "epoch_count = 0\n",
        "# function to front the characters that match an array of indices\n",
        "def print_chars(indexArray):\n",
        "    string = \"\"\n",
        "    for c in indexArray:\n",
        "        if (c != None):\n",
        "            string += ix_to_char.get(c)\n",
        "    print(string)\n",
        "    \n",
        "startChars = [\n",
        "    \"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\n",
        "    \"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\n",
        "    \"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\"\n",
        "]\n",
        "startSize = len(startChars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B6W8DB3b5aSs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Graph Creation\n",
        "In this graph, an LTSM cell is used rather than the basic RNN Cell. More specifically, LSTMBlockCell, which is optimized for CPU. LTSM stands for \"Long Short-Term Memory\". More information about LSTM cells can be found here: \n",
        "[LSTM Cell Api Docs](https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell/LSTMCell )"
      ]
    },
    {
      "metadata": {
        "id": "6ImKltoXqc_d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "import random\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# sets hyperparameters to mimic the Karpathy code\n",
        "n_neurons = 100\n",
        "num_batches = 10\n",
        "iterations = 200\n",
        "n_layers = 2\n",
        "learning_rate = 0.001\n",
        "sequence_to_use = 0\n",
        "num_sequences = X_modified.shape[0]\n",
        "epochs_run = 0\n",
        "\n",
        "# X has any number of batches and chars, and vocab_size due to one-hot encoding\n",
        "X = tf.placeholder(tf.float32, [None, None, vocab_size])\n",
        "\n",
        "# y has any number of batches, and length_of_input characters\n",
        "y = tf.placeholder(tf.int32, [None, length_of_input])\n",
        "\n",
        "# more TensorFlow stuff defined here\n",
        "layers = [tf.contrib.rnn.LSTMBlockCell(num_units=n_neurons) for layer in range(n_layers)]\n",
        "multi_layer_cell = tf.contrib.rnn.MultiRNNCell(layers)\n",
        "outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, X, dtype=tf.float32)\n",
        "logits = tf.layers.dense(outputs, vocab_size)\n",
        "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
        "loss = tf.reduce_mean(xentropy)\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "training_op = optimizer.minimize(loss)\n",
        "probs = tf.nn.softmax(logits)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "naE2Gqsx5zkA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Session Execution"
      ]
    },
    {
      "metadata": {
        "id": "FEQFHopw1Bx9",
        "colab_type": "code",
        "outputId": "3080fed1-efe8-4b25-b2df-f19e965910b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3046
        }
      },
      "cell_type": "code",
      "source": [
        "init = tf.global_variables_initializer()\n",
        "print_every = 100\n",
        "with tf.Session() as sess:\n",
        "    init.run()\n",
        "    \n",
        "    print(\"Encoding started...\")\n",
        "    X_encoded = tf.one_hot(X_modified, vocab_size)\n",
        "    X_encoded = X_encoded.eval()\n",
        "    print(\"Encoding done!\")\n",
        "    \n",
        "    # epoch loop starts here\n",
        "    while (True):\n",
        "        \n",
        "        if (epoch_count % print_every == 0):\n",
        "            # creates a seed to start the string from\n",
        "            pred_indices = [char_to_ix.get(startChars[random.randint(0,startSize-1)])]\n",
        "        \n",
        "        # each iteration is one new character\n",
        "        for iteraton in range(0, iterations):\n",
        "            \n",
        "            # creates batches, each row is an array of sequential characters\n",
        "            in_indices = []\n",
        "            out_indices = []\n",
        "            for batch in range(0, num_batches):\n",
        "            \n",
        "                # if we run out of sequences, the sequence to use returns to 0\n",
        "                if (sequence_to_use >= num_sequences):\n",
        "                    sequence_to_use = 0\n",
        "                    \n",
        "                # in and out indices are appended to\n",
        "                # 'sequence_to_use' is incrimented to get the next sequence when re-run\n",
        "                in_indices.append(X_encoded[sequence_to_use])\n",
        "                out_indices.append(y_modified[sequence_to_use])\n",
        "                sequence_to_use += 1\n",
        "            \n",
        "            # run the trainining op\n",
        "            sess.run(training_op, feed_dict={X: np.asarray(in_indices), y: np.asarray(out_indices)})\n",
        "        \n",
        "            if (epoch_count % print_every == 0):\n",
        "                # one hot encode the prediction indices\n",
        "                pred_encoded = tf.one_hot(np.asarray(pred_indices), vocab_size).eval()\n",
        "                \n",
        "                # get predictions as probabilities\n",
        "                predictions = sess.run(probs, feed_dict={X: np.asarray([pred_encoded])})\n",
        "                \n",
        "                # take the probabilities from the last character\n",
        "                # pick the next index using the probabilities\n",
        "                ix = np.random.choice(range(vocab_size), p=(predictions[0][-1]).ravel())\n",
        "                \n",
        "                # add to the array of indices\n",
        "                pred_indices.append(ix)\n",
        "                \n",
        "        if (epoch_count % print_every == 0):\n",
        "            # print the string every epoch for the first 'iterations_to_print' epochs\n",
        "            print(\"\\nEpoch:\", epoch_count,\"---------------------------------------------\")\n",
        "            print_chars(pred_indices)\n",
        "        \n",
        "        # increment the epoch count used to see when to print the results\n",
        "        epoch_count = epoch_count + 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoding started...\n",
            "Encoding done!\n",
            "\n",
            "Epoch: 0 ---------------------------------------------\n",
            "NsWDpoxU,F1gn,6jtmEuw  , tey   tt    tcsBot id mhh lcseansdaimBrd tds  .wn rwesh  in  reyiieer tlahet o etreoaimud ereneust.edrssosdidtnt tuyndteasy rsei ity r ihd rdnolaafh  y nDloy uyuoeoyl rr'aDo hh\n",
            "\n",
            "Epoch: 100 ---------------------------------------------\n",
            "At mile glaim,\" \n",
            "Professor Fbrong dorn and nape it as an up a poind in in on the oxle romess was palled up ever at fon Potter is is is ands. \n",
            "Gleffants, but it's pikering drice?\" se gate she reeping \n",
            "\n",
            "Epoch: 200 ---------------------------------------------\n",
            "The bully,\" sait Harry. \n",
            "\"At was ening if their wordetionly a fack. He robecing his head and sured... he boked him from with thing to that Peevenge befo the mirecting reoply at her a spen the around t\n",
            "\n",
            "Epoch: 300 ---------------------------------------------\n",
            "Hermione afred into pale sweetes sout over expects. Sinside as to gran's onved out of me so tiekhctory's going to steared, but I went and side up You hing needs and -- 'lid on the carge lack over the o\n",
            "\n",
            "Epoch: 400 ---------------------------------------------\n",
            "X Ctang throw, and were in for trying at them of the ted a pocker, went it?\" said Hagrid, then a Nevor brave his lands came stuff and place nclampor, but ble work -- it'll word over him pand pack about\n",
            "\n",
            "Epoch: 500 ---------------------------------------------\n",
            "Z!\" \n",
            "\"I was 'ven's to get sounded un-pumps? Grof pair things,\" said a books -- ! Harry for hurt-way. Anyway, would a lot wlerve wouldly -- silves to naid, but yeh roles Musally, wizausin' to Hagrid. H\n",
            "\n",
            "Epoch: 600 ---------------------------------------------\n",
            "Very practoa with than nos, I fered. \"That' Fill one we're mints's caked Harry, Harry. \"I hend him, to her recent the really they'd just perhay, the dust onto yen, and somework at Harry outions apartin\n",
            "\n",
            "Epoch: 700 ---------------------------------------------\n",
            "Cause Vernon said Uncle Vernon airnocan. who you,\" he secan's,\" he thought he shout another fast past it world get the bod out up found in cold anyt Dumbledore. \n",
            "THE URL CHELT GOT OUL! GOF SHO \n",
            "WHH\" \n",
            "\n",
            "Epoch: 800 ---------------------------------------------\n",
            "Quidditch of courth, the Slytherin angrision. \n",
            "\"its nose it was hugging up at Hogwarts whattered tention field, Hermione heard held Hermione reached Harry. The think which was cold this believe and du\n",
            "\n",
            "Epoch: 900 ---------------------------------------------\n",
            "Weare, quiver bravance after use on it master ghosts -- trise on the keeped notes perfect cad dnogbeing to chanking Praphange that Snape'll everyone -- a loud if his ands was. \n",
            "\"Yes,\" shair different \n",
            "\n",
            "Epoch: 1000 ---------------------------------------------\n",
            "Dudley shif gently. \n",
            "\"I want you as, wained they wouldn't happened, happened at door. Harry cnutter. \n",
            "\"Filch elf into Neville, it,\" he had talking them all three gamely. \n",
            "\"Slytherin again, Peeves, c\n",
            "\n",
            "Epoch: 1100 ---------------------------------------------\n",
            "Pates, who have been in the invise in his playing choose and swigh more head to hear eyes.\" \n",
            "\"LARVEN \n",
            "Hool two a suggest to find its Word his is.\" \n",
            "\"We find a bid oven the silence. \n",
            "There could hav\n",
            "\n",
            "Epoch: 1200 ---------------------------------------------\n",
            "ZWhat had she vastued the door. \n",
            "They the words look a long end off -- Uncle jerked to doith off in flow!\" \n",
            "\"Nodle stop into his facince? Harry worked to find Harry pull make you does that tlote for \n",
            "\n",
            "Epoch: 1300 ---------------------------------------------\n",
            "McGonagall'y deep you what get that on quite as if he had ever it it, snow, Harry's bittlents of to the mirrors!\" \n",
            "\"Professor, that's hurried and got portrait of this and stepped arten must have after\n",
            "\n",
            "Epoch: 1400 ---------------------------------------------\n",
            "Be'll be standing awful five freaks best selves leapt he said found a lotias to do, it was even can't say all these wide Bgistking the ilver started. There, hoge of them who you know up whe's dark arou\n",
            "\n",
            "Epoch: 1500 ---------------------------------------------\n",
            "Nov the large in the air and the dark, because Norbert, nack left sorry -- Norbert,\" Harry could be they some of rest our lotted at poaling dragon -- when they swoop in that it's doging at the constart\n",
            "\n",
            "Epoch: 1600 ---------------------------------------------\n",
            "Yeh this  -- Exachis well lettening in his usually everyone,\" said Hagrid ever way his note. The vianing into the shop e' came proud.\" \n",
            "Harry right hoped-but all windowsl't it wanted to get brogss. \"N\n",
            "\n",
            "Epoch: 1700 ---------------------------------------------\n",
            "Everywhared toward the Restricted back... this bendren appeared. \"Shaking away?\" Harry looked and Separed?\" \n",
            "\"No -- though, you mell kill you.\" \n",
            "\"I do we can -- spoke them. \"A you have eating. \n",
            "\"All\n",
            "\n",
            "Epoch: 1800 ---------------------------------------------\n",
            "Yet thank how his get him. His bite prouded the remembing about his Smelting Aunt Petunia, things was ag youps took be name. \n",
            "Make when never one through the dulder motered. He day to the house delive\n",
            "\n",
            "Epoch: 1900 ---------------------------------------------\n",
            "Do you wich painfall better over the snous I the Quidditch fire. I'm tible; she was a shates. The -- he's turned the restActing the walled Harry, any book. \n",
            "Nem. Hermione over Crabbe or I Hall leaving\n",
            "\n",
            "Epoch: 2000 ---------------------------------------------\n",
            "Voldemort colleciob, what've eleven more. His after her Mumbledore being a sat Paturcer half, you're taking hunw of Mrs. Dursley was a slapped out that was off to had the morened from tears the darks' \n",
            "\n",
            "Epoch: 2100 ---------------------------------------------\n",
            "Everyone bering train.\" \n",
            "Hermione's laken-Pordy-the traFed the train facent at odd hile. \n",
            "\"MAME!\" said Professor McGonagall gave it. Norrightenedly.\" \n",
            "It was afraid she'd had hald for for  Earmaty s\n",
            "\n",
            "Epoch: 2200 ---------------------------------------------\n",
            "He giving the other to recluiving out a for anythine free to brows seven wrong rudes?... not to the olded across to only many,\" said Harry looked out on urcycured at the linow. \n",
            "Snape -- long, and the\n",
            "\n",
            "Epoch: 2300 ---------------------------------------------\n",
            "EDIF-: MOR'RHEBINDOR,\" Ron and \n",
            "The walloped to as inside the dark with his landed at him from the began right the round through a took, Will be aboat and Dean was sitting! Yes --\" \n",
            "Professor Dumblam\n",
            "\n",
            "Epoch: 2400 ---------------------------------------------\n",
            "XNIREING; Firenze as though the pale hurrieding you like a heart I should think you can Fang't just think flow almost refere around. And Paturney. \n",
            "Harry was clearing like neat new stay through the re\n",
            "\n",
            "Epoch: 2500 ---------------------------------------------\n",
            "Good, and you was why they don' if yew he couldn't won' er never haw, desperate his start, looking tearming curty tight great and the boy, in his crorbling corn hair. The worth, you force our compared.\n",
            "\n",
            "Epoch: 2600 ---------------------------------------------\n",
            "Very last middle. He then the same would speak in these in the last on. \"He he seemed to say yeh know -- reflecting Hermione. \n",
            "\"Reworted. \"I thought see in a where, Merry and prace only ilver grate an\n",
            "\n",
            "Epoch: 2700 ---------------------------------------------\n",
            "But \"It my hut, two after Hagrid, lookin on the next sort of completely for side. He was them.\" \n",
            "Hagrid so it will half-head. \"MEbR\" \n",
            "I was taking into magic, seemed caiter the glasses --\" \n",
            "\"Yet was\n",
            "\n",
            "Epoch: 2800 ---------------------------------------------\n",
            "CPowncalked quite locked said Ron glad quickly.\" \n",
            "\"You know afternoon; anyone. What isn't leave your salfed on, too.\" \n",
            "The sight on his fisting bably both called voice. \n",
            "\"It's girng any points -- Ge\n",
            "\n",
            "Epoch: 2900 ---------------------------------------------\n",
            "CTASNOF HALICLOY!\" \n",
            "Uncle Vernon ince had rewer, Snape just it evomber already watch to remember your gentraed and scrawld in the parents next been at ppeared his remmust let you were swined at the ow\n",
            "\n",
            "Epoch: 3000 ---------------------------------------------\n",
            "Ron -- he'll be playort to go of homedore with the dncing already them. \n",
            "\"You're on Alain candlemered feet their very sented this moon. Malfoy cold, Harrid all their prefects. Dumbledore an empty duck\n",
            "\n",
            "Epoch: 3100 ---------------------------------------------\n",
            "Walking at the pointing in the didry at that Stone and Grunning it was expriet, let wrist by. \n",
            "\"Hurney there was form of Gringotts that seemed to be a curse that he true in his back at it, and Quidden\n",
            "\n",
            "Epoch: 3200 ---------------------------------------------\n",
            "Pretall?\" he said asleep, turned robes something, Harry was going to -- I watchage them hearing to the plate dog night nose to be job to in one families! How drew wand, obviout to him to learning onto \n",
            "\n",
            "Epoch: 3300 ---------------------------------------------\n",
            "FIGL FOOKS LAS abonom?\" \n",
            "The key dog's gleaning around the horse would have empty' examined we're the ghost, I can't think case off the supposed them. It heredned, hardly stepped bed. \n",
            "\"Nope that mea\n",
            "\n",
            "Epoch: 3400 ---------------------------------------------\n",
            "High Ron.  hat back into the secaet of on pocurnels and 'em out of the cold as Hagrid's jacks again. The crowd families. \n",
            "\"What all -- I never fat grin those Granger hundred wasn't the first thing at \n",
            "\n",
            "Epoch: 3500 ---------------------------------------------\n",
            "In it the forest ter unicorn's throwing where as they're yer wandd it going... wild that doing about Mr. Neewhen' Can new hurtant a moment hut, who's Mr. Conside lines of body, they warn, and went to t\n",
            "\n",
            "Epoch: 3600 ---------------------------------------------\n",
            "Much. \n",
            "They had sense, head had just say he had the slow, -- \"I want to skew like never had usen We're glettimes that uncle are.\" \n",
            "Harry was that youwdee, burres wore flickering all. It knock at the \n",
            "\n",
            "Epoch: 3700 ---------------------------------------------\n",
            "Ron stood her hundred and Ron found out who they muttered white earsz we wanted to, unwalp he pulled teeth to Goylon tones after the walls at the slipped his pillow? But Snape is yeh, or some possibly \n",
            "\n",
            "Epoch: 3800 ---------------------------------------------\n",
            "RBOLDES POANTqORIIT RY! \n",
            "CHWO- HHj.\" \n",
            "Aunt Petunia ages of cighten his burst inside. Torts -- I amis pocket of a moment out about a course. He was out of Nevisty, the stamp pocket in the large clothe\n",
            "\n",
            "Epoch: 3900 ---------------------------------------------\n",
            "Thousands of the troll birthdan three person what almost but Harry was shrip up on. \n",
            "\"Slytherin -- I start happened curse came around his knees, none of him. So he'd be rush and pie. \n",
            "Snape Keeper, s\n",
            "\n",
            "Epoch: 4000 ---------------------------------------------\n",
            "X-OUD. \n",
            "When two ount back. \n",
            "Avaurchier just as he had enough of Prive! Done on his family into the air oviving, \"she whip\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}